---
title: "ENVS-193DS Final"
author: "Rebecca Martinez"
date: "June 12, 2025"
format: html
---

ðŸ’» [**GitHub Repository**](https://github.com/RebeccaLMartinez/ENVS-193DS_spring-2025_final)

```{r setup,  message=FALSE, warning=FALSE, error=FALSE}

# --- necessary packages (possibly more) ---

library(tidyverse)   # General data manipulation and visualization
library(here)        # Simplifies file path management
library(janitor)     # Helpful functions to clean data frames
library(readxl)      # Read Excel files
library(scales)      # Customize axis labels in plots
library(ggeffects)   # Generate model predictions for plotting
library(flextable)   # Create polished tables for reports
library(rstatix)     # Easy statistical tests
library(effectsize)  # Calculate effect sizes for models
library(lubridate)   # Simplifies working with dates

# --- sea surface temperature data ---

sst <- read_csv(here("data", "sst_update2023.csv"))  # SST data CSV file

```

# Problem 1. Research writing 

## a. Transparent statistical methods  

**In part 1**, a linear regression test was likely used because both distance from headwater and nitrogen load are continuous variables. The p-value of 0.03 indicates that the slope of the relationship between distance and nitrogen load is significantly different from zero, meaning there is a statistically significant association between these variables. **In part 2**, average nitrogen loads across five different sources (urban land, atmospheric deposition, fertilizer, wastewater treatment, and grasslands), which suggests they used a one-way ANOVA, the standard method for comparing means across multiple categorical groups. The p-value of 0.02 indicates at least one group mean differs significantly from the others.  

Because no other details were provided, these are the most likely tests based on the language and p-values reported.  

## b. More information needed  

The test in part 2 looks like an ANOVA, which is good for comparing nitrogen load across sources. But to give more context, my co-worker should add these two things:

1. **Post-hoc test (like Tukeyâ€™s HSD)**. ANOVA tells us that at least one group is different, but not which ones. A post-hoc test helps identify which specific sources differ. 

2. **Assumptions checks (like Shapiro-Wilk for normality  and Leveneâ€™s test for equal variance)**. ANOVA assumes the nitrogen load data is normally distributed within each group and that the variances are similar. Checking this helps make sure the ANOVA results are valid. If the assumptions arenâ€™t met, a different test like Kruskal-Wallis might be better.

Adding these steps would improve the strength of the conclusions and lower the chance of drawing the wrong conclusions from the data.

## c. Suggestions for rewriting 






# Problem 2

Below you can find code to clean and summarize the 'sst' data set:

```{r cleaning data}


sst_clean <- sst |> # new df using sst data
  # Convert data types
  mutate(
    date = as.Date(date, format = "%Y-%m-%d"),   # Format date as YYYY-MM-DD
    site = as.character(site),                   # Make site a character
    latitude = as.numeric(latitude),             # Make latitude numeric
    longitude = as.numeric(longitude),           # Make longitude numeric
    temp = as.numeric(temp)                      # Make temp numeric
  ) |>

  # Create year and month columns
  mutate(
    year = factor(format(date, "%Y"), levels = as.character(2018:2023)),  # Year as factor
    month = factor(format(date, "%b"), levels = month.abb, ordered = TRUE)  # Ordered month names
  ) |>
 drop_na() |>  # Remove rows with any missing values

  group_by(year, month) |>  # Group data by year and month

  summarise(
    mean_monthly_sst = mean(temp),  # Calculate mean temp for each group
    .groups = "drop"                # Ungroup after summarizing
  )

```


```{r data structure}

slice_sample(
  sst_clean,
  n = 5)

str(sst_clean)



```

